{"cells":[{"cell_type":"markdown","metadata":{},"source":["### How to run this notebook"]},{"cell_type":"markdown","metadata":{},"source":["I used kaggle for this notebook so to run it with out any need of changing the file paths, you need to use kaggle. Make sure you have a GPU quota turned on. Then you need to clik on run all to run every cell in the notebook and wait a couple of hours as notebook runs. "]},{"cell_type":"markdown","metadata":{},"source":["### hyperparameters used for the notebook"]},{"cell_type":"markdown","metadata":{},"source":["After loading the dataset by data loaders and building the model, the model was designed with 3 layers. we then trained the model for about 75 epochs and using learning rate of 2e-3, batch size of 64, using dropout of 0.2, lstm dropout of 0.25 and decoder dropout of 0.15, and encoder dropout of 0.3. In addition, we used CTCLoss as the criterion and we used Adam optimer for the decoder we employed CTCBeamDecoder and for the scheduler, we utilized ReducedOnPlateua with mode set to min mode and factor of 0.8, for the patience, I started with using 3 and I decreased along the running process until it eventually became 1 at the end. Gradscaler was used too for scaling. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["After running the notebook the last optimal result that was obtained in 4.0095 on notebook but on making submission to kaggle the notebook scored 4.41"]},{"cell_type":"markdown","metadata":{},"source":["# Installs"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:13:43.279139Z","iopub.status.busy":"2024-03-23T03:13:43.278800Z","iopub.status.idle":"2024-03-23T03:15:38.711567Z","shell.execute_reply":"2024-03-23T03:15:38.710393Z","shell.execute_reply.started":"2024-03-23T03:13:43.279110Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117 -q"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T20:39:14.849752Z","iopub.status.busy":"2024-03-22T20:39:14.849396Z","iopub.status.idle":"2024-03-22T20:39:27.191623Z","shell.execute_reply":"2024-03-22T20:39:27.190423Z","shell.execute_reply.started":"2024-03-22T20:39:14.849717Z"},"trusted":true},"outputs":[],"source":["!pip install torch-summary wandb  --quiet   #!pip install torchsummaryX wandb --quiet"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:20:00.585495Z","iopub.status.busy":"2024-03-23T03:20:00.584653Z","iopub.status.idle":"2024-03-23T03:21:00.621981Z","shell.execute_reply":"2024-03-23T03:21:00.620708Z","shell.execute_reply.started":"2024-03-23T03:20:00.585461Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'ctcdecode' already exists and is not an empty directory.\n","/kaggle/working/ctcdecode\n","/kaggle/working\n"]}],"source":["!pip install wandb --quiet\n","!pip install python-Levenshtein -q\n","!git clone --recursive https://github.com/parlance/ctcdecode.git\n","!pip install wget -q\n","%cd ctcdecode\n","!pip install . -q\n","%cd ..\n","\n","#!pip install torchsummaryX -q\n","#from torchsummary import summary"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-23T03:18:25.535663Z","iopub.status.idle":"2024-03-23T03:18:25.536071Z","shell.execute_reply":"2024-03-23T03:18:25.535897Z","shell.execute_reply.started":"2024-03-23T03:18:25.535875Z"},"trusted":true},"outputs":[],"source":["'''\n","If torchsummaryX doesn't work, please run this cell. Alternatively, please refer to Piazza post @209 for more assistance:\n","'''\n","\n","!pip install torchsummaryx==1.1.0"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:22:32.432803Z","iopub.status.busy":"2024-03-23T03:22:32.431950Z","iopub.status.idle":"2024-03-23T03:22:34.724992Z","shell.execute_reply":"2024-03-23T03:22:34.723869Z","shell.execute_reply.started":"2024-03-23T03:22:32.432766Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device:  cuda\n"]}],"source":["import torch\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","#from torchsummaryX import summary\n","#from torchsummary import summary\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","\n","import torchaudio.transforms as tat\n","\n","from sklearn.metrics import accuracy_score\n","import gc\n","\n","import zipfile\n","#import pandas as pd\n","from tqdm import tqdm\n","import os\n","import datetime\n","\n","# imports for decoding and distance calculation\n","import ctcdecode\n","import Levenshtein\n","from ctcdecode import CTCBeamDecoder\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", device)"]},{"cell_type":"markdown","metadata":{},"source":["# Kaggle Setup"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:22:39.551449Z","iopub.status.busy":"2024-03-23T03:22:39.550531Z","iopub.status.idle":"2024-03-23T03:22:46.376319Z","shell.execute_reply":"2024-03-23T03:22:46.374935Z","shell.execute_reply.started":"2024-03-23T03:22:39.551415Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting kaggle==1.5.8\n","  Downloading kaggle-1.5.8.tar.gz (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m970.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73248 sha256=8bcd435dbd01cd3f5c9a4b56e274bded02c314e39b2d2c5198c53c3f8145c40c\n","  Stored in directory: /root/.cache/pip/wheels/0b/76/ca/e58f8afa83166a0e68f0d5cd2e7f99d260bdc40e35da080eee\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.6.6\n","    Uninstalling kaggle-1.6.6:\n","      Successfully uninstalled kaggle-1.6.6\n","Successfully installed kaggle-1.5.8\n"]}],"source":["# TODO: Use the same Kaggle code from HW1P2\n","\n","!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n","!mkdir /root/.kaggle\n","\n","with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n","    f.write('{\"username\":\"leonard250\",\"key\":\"2f9461ced39b0687292591bf1315ae7a\"}')\n","    # Put your kaggle username & key here\n","\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:22:47.418734Z","iopub.status.busy":"2024-03-23T03:22:47.417648Z","iopub.status.idle":"2024-03-23T03:23:07.056028Z","shell.execute_reply":"2024-03-23T03:23:07.054810Z","shell.execute_reply.started":"2024-03-23T03:22:47.418694Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading hw3p2asr-s24.zip to /kaggle/working\n","100%|██████████████████████████████████████▉| 3.73G/3.74G [00:17<00:00, 240MB/s]\n","100%|███████████████████████████████████████| 3.74G/3.74G [00:17<00:00, 228MB/s]\n"]}],"source":["!kaggle competitions download -c hw3p2asr-s24"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:23:07.059505Z","iopub.status.busy":"2024-03-23T03:23:07.058528Z","iopub.status.idle":"2024-03-23T03:23:58.612036Z","shell.execute_reply":"2024-03-23T03:23:58.610875Z","shell.execute_reply.started":"2024-03-23T03:23:07.059465Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["11-785-s24-hw3p2  ctcdecode  hw3p2asr-s24.zip\n"]}],"source":["'''\n","This will take a couple minutes, but you should see at least the following:\n","11-785-s24-hw3p2  ctcdecode  hw3p2asr-s24.zip  sample_data\n","'''\n","!unzip -q hw3p2asr-s24.zip\n","!ls"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:23:58.614229Z","iopub.status.busy":"2024-03-23T03:23:58.613814Z","iopub.status.idle":"2024-03-23T03:24:14.825185Z","shell.execute_reply":"2024-03-23T03:24:14.824087Z","shell.execute_reply.started":"2024-03-23T03:23:58.614184Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting speechpy\n","  Downloading speechpy-2.4-py2.py3-none-any.whl.metadata (407 bytes)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from speechpy) (1.11.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from speechpy) (1.26.4)\n","Downloading speechpy-2.4-py2.py3-none-any.whl (9.5 kB)\n","Installing collected packages: speechpy\n","Successfully installed speechpy-2.4\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install speechpy"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:24:14.854188Z","iopub.status.busy":"2024-03-23T03:24:14.853604Z","iopub.status.idle":"2024-03-23T03:24:14.858211Z","shell.execute_reply":"2024-03-23T03:24:14.857323Z","shell.execute_reply.started":"2024-03-23T03:24:14.854151Z"},"trusted":true},"outputs":[],"source":["import speechpy\n","import glob"]},{"cell_type":"markdown","metadata":{},"source":["# Google Drive"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T13:30:33.670785Z","iopub.status.busy":"2024-03-22T13:30:33.669647Z","iopub.status.idle":"2024-03-22T13:30:33.675100Z","shell.execute_reply":"2024-03-22T13:30:33.674255Z","shell.execute_reply.started":"2024-03-22T13:30:33.670755Z"},"trusted":true},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset and Dataloader"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:24:14.860183Z","iopub.status.busy":"2024-03-23T03:24:14.859579Z","iopub.status.idle":"2024-03-23T03:24:14.869155Z","shell.execute_reply":"2024-03-23T03:24:14.868223Z","shell.execute_reply.started":"2024-03-23T03:24:14.860150Z"},"trusted":true},"outputs":[],"source":["# ARPABET PHONEME MAPPING\n","# DO NOT CHANGE\n","\n","CMUdict_ARPAbet = {\n","    \"\" : \" \",\n","    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\",\n","    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\",\n","    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\",\n","    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\",\n","    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\",\n","    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\",\n","    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\",\n","    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n","    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n","}\n","\n","CMUdict = list(CMUdict_ARPAbet.keys())\n","ARPAbet = list(CMUdict_ARPAbet.values())\n","\n","\n","PHONEMES = CMUdict[:-2]\n","LABELS = ARPAbet[:-2]"]},{"cell_type":"markdown","metadata":{},"source":["### Train Data"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:41:20.618229Z","iopub.status.busy":"2024-03-23T03:41:20.617761Z","iopub.status.idle":"2024-03-23T03:41:20.638925Z","shell.execute_reply":"2024-03-23T03:41:20.637897Z","shell.execute_reply.started":"2024-03-23T03:41:20.618197Z"},"trusted":true},"outputs":[],"source":["class AudioDataset(torch.utils.data.Dataset):\n","\n","    # For this homework, we give you full flexibility to design your data set class.\n","    # Hint: The data from HW1 is very similar to this HW\n","\n","    #TODO\n","    def __init__(self,root_dir,partition,transforms,limit=None,train_460=True): \n","        '''\n","        Initializes the dataset.\n","\n","        INPUTS: What inputs do you need here?\n","        '''\n","\n","        # Load the directory and all files in them\n","\n","        self.mfcc_dir = f\"{root_dir}/{partition}*/mfcc/*\" #TODO.     # '*' for regex\n","        self.transcript_dir = f\"{root_dir}/{partition}*/transcript/*\"#TODO\n","\n","        # self.mfcc_files = os.listdir(self.mfcc_dir) #TODO\n","        # self.transcript_files = os.listdir(self.mfcc_dir) #TODO\n","\n","        self.mfcc_files =sorted(glob.glob(self.mfcc_dir))\n","        self.transcript_files= sorted(glob.glob(self.transcript_dir))\n","\n","        #limit for partial data\n","        \n","        self.phonemes = PHONEMES\n","\n","        assert len(self.mfcc_files) == len(self.transcript_files) \n","\n","        self.mfccs, self.transcripts = [], []\n","\n","        for file in self.mfcc_files:\n","          self.mfccs.append(np.load(file))\n","        for file in self.transcript_files:\n","          temp =np.load(file)[1:-1]\n","          self.transcripts.append(np.array([self.phonemes.index(i) for i in temp]))\n","        \n","        print('before train 360 self.mfccs len =',len(self.mfccs))\n","\n","        # for train 460 data\n","        if train_460:\n","          partition_460 = 'train-clean-360'\n","          self.mfcc_files_460= sorted(glob.glob(f\"{root_dir}/{partition_460}*/mfcc/*\"))\n","          self.transcript_files_460= sorted(glob.glob(f\"{root_dir}/{partition_460}*/transcript/*\"))\n","\n","\n","          for file in self.mfcc_files_460:\n","            self.mfccs.append(np.load(file))\n","          for file in self.transcript_files_460:\n","            temp =np.load(file)[1:-1]\n","            self.transcripts.append(np.array([self.phonemes.index(i) for i in temp]))\n","        \n","        print('after train 360 self.mfccs len =',len(self.mfccs))\n","          \n","        #TODO\n","        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n","        self.length = len(self.mfccs)\n","\n","        assert len(self.mfccs) == len(self.transcripts)\n","\n","        print('AUDIODATASET LOADED ...')\n","        \n","        #TODO\n","        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n","        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n","\n","        #TODO\n","        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n","        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n","        '''\n","        You may decide to do this in __getitem__ if you wish.\n","        However, doing this here will make the __init__ function take the load of\n","        loading the data, and shift it away from training.\n","        '''\n","       \n","\n","    def __len__(self):\n","        \n","        '''\n","        TODO: What do we return here?\n","        '''\n","        return self.length\n","        # raise NotImplemented\n","\n","    def __getitem__(self, ind):\n","        '''\n","        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n","\n","        If you didn't do the loading and processing of the data in __init__,\n","        do that here.\n","\n","        Once done, return a tuple of features and labels.\n","        '''\n","        mfcc = self.mfccs[ind]\n","        transcript = self.transcripts[ind]\n","\n","        return torch.FloatTensor(mfcc), torch.tensor(transcript)\n","        # raise NotImplemented\n","\n","\n","    def collate_fn(self,batch):\n","        '''\n","        TODO:\n","        1.  Extract the features and labels from 'batch'\n","        2.  We will additionally need to pad both features and labels,\n","            look at pytorch's docs for pad_sequence\n","        3.  This is a good place to perform transforms, if you so wish. \n","            Performing them on batches will speed the process up a bit.\n","        4.  Return batch of features, labels, lenghts of features, \n","            and lengths of labels.\n","        '''\n","        # batch of input mfcc coefficients\n","        batch_mfcc = [i[0] for i in batch] # TODO\n","        # batch of output phonemes\n","        batch_transcript = [i[1] for i in batch] # TODO\n","\n","        # HINT: CHECK OUT -> pad_sequence (imported above)\n","        # Also be sure to check the input format (batch_first)\n","        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True) # TODO\n","        lengths_mfcc = [len(i) for i in batch_mfcc] # TODO \n","\n","        batch_transcript_pad = pad_sequence(batch_transcript, batch_first = True) # TODO\n","        lengths_transcript = [len(i) for i  in batch_transcript] # TODO\n","\n","        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n","        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n","        #                  -> Would we apply transformation on the validation set as well?\n","        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n","        \n","        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n","        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n","\n","       "]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Test Data"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:41:25.952514Z","iopub.status.busy":"2024-03-23T03:41:25.951849Z","iopub.status.idle":"2024-03-23T03:41:25.963383Z","shell.execute_reply":"2024-03-23T03:41:25.962215Z","shell.execute_reply.started":"2024-03-23T03:41:25.952481Z"},"trusted":true},"outputs":[],"source":["class AudioTestDataset(torch.utils.data.Dataset):\n","\n","    # For this homework, we give you full flexibility to design your data set class.\n","    # Hint: The data from HW1 is very similar to this HW\n","\n","    #TODO\n","    def __init__(self,root_dir,partition,transforms,limit=None): \n","        '''\n","        Initializes the dataset.\n","\n","        INPUTS: What inputs do you need here?\n","        '''\n","\n","        # Load the directory and all files in them\n","\n","        self.mfcc_dir = f\"{root_dir}/{partition}*/mfcc/*\" #TODO.     # '*' for regex\n","\n","        # self.mfcc_files = os.listdir(self.mfcc_dir) #TODO\n","        # self.transcript_files = os.listdir(self.mfcc_dir) #TODO\n","\n","        self.mfcc_files =sorted(glob.glob(self.mfcc_dir))\n","\n","        self.PHONEMES = PHONEMES\n","\n","        self.mfccs = []\n","\n","        for files in self.mfcc_files:\n","          self.mfccs.append(np.load(files))\n","\n","        print('AUDIODATASET TEST LOADED ...')\n","\n","        #TODO\n","        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n","        self.length = len(self.mfcc_files)\n","        \n","        #TODO\n","        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n","        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n","\n","        #TODO\n","        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n","        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n","        '''\n","        You may decide to do this in __getitem__ if you wish.\n","        However, doing this here will make the __init__ function take the load of\n","        loading the data, and shift it away from training.\n","        '''\n","       \n","\n","    def __len__(self):\n","        \n","        '''\n","        TODO: What do we return here?\n","        '''\n","        return self.length\n","        # raise NotImplemented\n","\n","    def __getitem__(self, ind):\n","        '''\n","        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n","\n","        If you didn't do the loading and processing of the data in __init__,\n","        do that here.\n","\n","        Once done, return a tuple of features and labels.\n","        '''\n","        mfcc = self.mfccs[ind]\n","\n","        return torch.FloatTensor(mfcc)\n","        # raise NotImplemented\n","\n","\n","    def collate_fn(self,batch):\n","        '''\n","        TODO:\n","        1.  Extract the features and labels from 'batch'\n","        2.  We will additionally need to pad both features and labels,\n","            look at pytorch's docs for pad_sequence\n","        3.  This is a good place to perform transforms, if you so wish. \n","            Performing them on batches will speed the process up a bit.\n","        4.  Return batch of features, labels, lenghts of features, \n","            and lengths of labels.\n","        '''\n","        # batch of input mfcc coefficients\n","        batch_mfcc = batch # TODO\n","\n","        # HINT: CHECK OUT -> pad_sequence (imported above)\n","        # Also be sure to check the input format (batch_first)\n","        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True) # TODO\n","        lengths_mfcc = [len(i) for i in batch_mfcc] # TODO \n","\n","        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n","        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n","        #                  -> Would we apply transformation on the validation set as well?\n","        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n","        \n","        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n","        return batch_mfcc_pad, torch.tensor(lengths_mfcc)\n","\n","       "]},{"cell_type":"markdown","metadata":{},"source":["### Config - Hyperparameters"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T06:12:35.951578Z","iopub.status.busy":"2024-03-23T06:12:35.951183Z","iopub.status.idle":"2024-03-23T06:12:35.960172Z","shell.execute_reply":"2024-03-23T06:12:35.959088Z","shell.execute_reply.started":"2024-03-23T06:12:35.951541Z"},"trusted":true},"outputs":[],"source":["BATCH_SIZE = 64\n","\n","transforms = []\n","\n","\n","# Feel free to add more items here\n","config = {\n","    \"beam_width\" : 2,\n","    \"lr\"         : 2e-3,\n","    \"epochs\"     :73,\n","    \"batch_size\" : 64,  # Increase if your device can handle it\n","    'sc_mode'    : 'min',\n","    'patience'   : 1,\n","    'encoder dropout': 0.3,\n","    'decoder dropout': 0.15,\n","    'lstm dropout'   : 0.25,\n","    'sc_threshold': 1e-4,\n","    'factor'     : 0.8,\n","    \"num_layers\": 3,\n","    'threshold mode': 'rel',\n","    \"dropout\": 0.2\n","    \n","}\n","\n","root = '/kaggle/working/11-785-s24-hw3p2/'\n","\n","# You may pass this as a parameter to the dataset class above\n","# This will help modularize your implementation\n","#transforms = [] # set of tranformations"]},{"cell_type":"markdown","metadata":{},"source":["### Data loaders"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:23:11.837173Z","iopub.status.busy":"2024-03-23T05:23:11.836799Z","iopub.status.idle":"2024-03-23T05:23:12.040877Z","shell.execute_reply":"2024-03-23T05:23:12.039735Z","shell.execute_reply.started":"2024-03-23T05:23:11.837145Z"},"trusted":true},"outputs":[{"data":{"text/plain":["526"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:23:15.706905Z","iopub.status.busy":"2024-03-23T05:23:15.706110Z","iopub.status.idle":"2024-03-23T05:23:30.824904Z","shell.execute_reply":"2024-03-23T05:23:30.823901Z","shell.execute_reply.started":"2024-03-23T05:23:15.706869Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["before train 360 self.mfccs len = 28539\n","after train 360 self.mfccs len = 28539\n","AUDIODATASET LOADED ...\n","before train 360 self.mfccs len = 2703\n","after train 360 self.mfccs len = 2703\n","AUDIODATASET LOADED ...\n","AUDIODATASET TEST LOADED ...\n","Batch size:  64\n","Train dataset samples = 28539, batches = 446\n","Val dataset samples = 2703, batches = 43\n","Test dataset samples = 2620, batches = 41\n"]}],"source":["# Create objects for the dataset class\n","train_data = AudioDataset(root_dir = root, partition='train-clean-100', transforms=[], limit=None,train_460=True) #TODO\n","val_data = AudioDataset(root_dir= root, partition='dev-clean', transforms=[], limit=None, train_460 = False ) # TODO : You can either use the same class with some modifications or make a new one :)\n","test_data = AudioTestDataset(root_dir=root, partition='test-clean', transforms=[], limit=None) #TODO\n","\n","# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n","train_loader = torch.utils.data.DataLoader(dataset = train_data, num_workers=7, batch_size =config['batch_size'],pin_memory=True, shuffle=True, collate_fn= train_data.collate_fn)#TODO\n","val_loader = torch.utils.data.DataLoader(dataset = val_data, num_workers=4, batch_size =config['batch_size'],pin_memory=True, shuffle=True, collate_fn= val_data.collate_fn)#TODO#TODO\n","test_loader = torch.utils.data.DataLoader(dataset = test_data, num_workers=1, batch_size =config['batch_size'],pin_memory=True, shuffle=False, collate_fn= test_data.collate_fn)#TODO#TODO\n","\n","print(\"Batch size: \", config['batch_size'])\n","print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n","print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n","print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:41:57.707439Z","iopub.status.busy":"2024-03-23T03:41:57.706854Z","iopub.status.idle":"2024-03-23T03:41:59.228089Z","shell.execute_reply":"2024-03-23T03:41:59.226908Z","shell.execute_reply.started":"2024-03-23T03:41:57.707404Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 1651, 27]) torch.Size([64, 203]) torch.Size([64]) torch.Size([64])\n"]}],"source":["# sanity check\n","for data in train_loader:\n","    x, y, lx, ly = data\n","    print(x.shape, y.shape, lx.shape, ly.shape)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["# NETWORK"]},{"cell_type":"markdown","metadata":{},"source":["## Basic\n","\n","This is a basic block for understanding, you can skip this and move to pBLSTM one"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:23:31.569687Z","iopub.status.busy":"2024-03-23T05:23:31.569022Z","iopub.status.idle":"2024-03-23T05:23:31.581303Z","shell.execute_reply":"2024-03-23T05:23:31.580308Z","shell.execute_reply.started":"2024-03-23T05:23:31.569654Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","class Network(nn.Module):\n","\n","    def __init__(self, hidden_size, nlayers, out_size=47, embed_size=40):\n","\n","        super(Network, self).__init__()\n","        self.nlayers = nlayers\n","        self.hidden_size = hidden_size\n","        self.embed_size = embed_size\n","        self.out_size = out_size\n","        self.cnns = torch.nn.Sequential(\n","            nn.Conv1d(self.embed_size, self.hidden_size, 3, padding=1, bias=False),\n","            nn.BatchNorm1d(self.hidden_size),\n","            nn.ReLU(inplace=True))\n","        # Adding some sort of embedding layer or feature extractor might help performance.\n","        # self.embedding = ?\n","        \n","        # TODO : look up the documentation. You might need to pass some additional parameters.\n","        self.lstm = nn.LSTM(input_size=self.hidden_size,\n","                            hidden_size=self.hidden_size,\n","                            num_layers=3,\n","                            bias=True,\n","                            batch_first=True,\n","                            dropout=0.2, # regularization\n","                            bidirectional=True) \n","       \n","        self.classification = nn.Sequential(\n","            #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n","            nn.Linear(self.hidden_size*2, self.hidden_size),\n","            nn.Linear(self.hidden_size, self.out_size)\n","            \n","        )\n","\n","        \n","        # self.logSoftmax = nn.log_softmax(2)#TODO: Apply a log softmax here. Which dimension would apply it on ?\n","        self.logSoftmax =  nn.LogSoftmax(2)\n","\n","    def forward(self, x, lx):\n","        #TODO\n","        # The forward function takes 2 parameter inputs here. Why?\n","        # Refer to the handout for hints\n","        x_cnn_input = x.permute(0, 2, 1) # (B, C_in, T_in)\n","        # x_cnn_input = x\n","        # x_cnn_input = x.transpose(1, 2)\n","        x_post_cnn = self.cnns(x_cnn_input) # (B, C_out, T_out)\n","        x_rnn_in = x_post_cnn.permute(2, 0, 1) # (T, B, C_out)\n","        # x_rnn_in = x_post_cnn\n","        # x_rnn_in = x_post_cnn.transpose(1, 2)\n","        x_packed = pack_padded_sequence(x_rnn_in, lx, enforce_sorted=False)\n","        out_packed, hidden = self.lstm(x_packed)\n","        out, out_lens = pad_packed_sequence(out_packed, batch_first=True) # (B, T, C)\n","        \n","        # Log softmax after output layer is required since nn.CTCLoss expect log prob\n","        out_prob = self.classification(out) # (B, T, Classes=47)\n","        out_prob = self.logSoftmax(out_prob)\n","        \n","        # Permute to fit for input format of CTCLoss\n","        # out_prob = out_prob.permute(1, 0, 2) #torch.transpose(out_prob, 0, 1) # (T, B, C)\n","        \n","        # TODO: calculate new xLens\n","        return out_prob, lx"]},{"cell_type":"markdown","metadata":{},"source":["## Initialize Basic Network\n","(If trying out the basic Network)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:41:59.247228Z","iopub.status.busy":"2024-03-23T03:41:59.246900Z","iopub.status.idle":"2024-03-23T03:42:12.077799Z","shell.execute_reply":"2024-03-23T03:42:12.076643Z","shell.execute_reply.started":"2024-03-23T03:41:59.247201Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch-summary in /opt/conda/lib/python3.10/site-packages (1.4.5)\n"]}],"source":["! pip install torch-summary"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:42:12.079465Z","iopub.status.busy":"2024-03-23T03:42:12.079184Z","iopub.status.idle":"2024-03-23T03:42:12.084358Z","shell.execute_reply":"2024-03-23T03:42:12.083352Z","shell.execute_reply.started":"2024-03-23T03:42:12.079440Z"},"trusted":true},"outputs":[],"source":["from torchsummary import summary\n"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:23:36.275196Z","iopub.status.busy":"2024-03-23T05:23:36.274453Z","iopub.status.idle":"2024-03-23T05:23:36.328091Z","shell.execute_reply":"2024-03-23T05:23:36.327243Z","shell.execute_reply.started":"2024-03-23T05:23:36.275159Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Network(\n","  (cnns): Sequential(\n","    (0): Conv1d(27, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n","    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (lstm): LSTM(256, 256, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n","  (classification): Sequential(\n","    (0): Linear(in_features=512, out_features=256, bias=True)\n","    (1): Linear(in_features=256, out_features=41, bias=True)\n","  )\n","  (logSoftmax): LogSoftmax(dim=2)\n",")\n"]}],"source":["# Assuming your model is named Network and is already defined\n","model = Network(hidden_size=256, nlayers=3, out_size=len(PHONEMES), embed_size=27).to(device)\n","print(model)\n","#summary(model, x.to(device), lx)\n","#summary(model, input_size=(27, ))  # Replace input_size with your actual input shape\n"]},{"cell_type":"markdown","metadata":{},"source":["## ASR Network"]},{"cell_type":"markdown","metadata":{},"source":["### Pyramid Bi-LSTM (pBLSTM)"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:23:39.152006Z","iopub.status.busy":"2024-03-23T05:23:39.151156Z","iopub.status.idle":"2024-03-23T05:23:39.158973Z","shell.execute_reply":"2024-03-23T05:23:39.157727Z","shell.execute_reply.started":"2024-03-23T05:23:39.151971Z"},"trusted":true},"outputs":[],"source":["# Utils for network\n","torch.cuda.empty_cache()\n","\n","class PermuteBlock(torch.nn.Module):\n","    def forward(self, x):\n","        return x.transpose(1, 2)"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:23:42.518966Z","iopub.status.busy":"2024-03-23T05:23:42.518256Z","iopub.status.idle":"2024-03-23T05:23:42.531875Z","shell.execute_reply":"2024-03-23T05:23:42.530793Z","shell.execute_reply.started":"2024-03-23T05:23:42.518930Z"},"trusted":true},"outputs":[],"source":["class pBLSTM(torch.nn.Module):\n","\n","    '''\n","    Pyramidal BiLSTM\n","    Read the write up/paper and understand the concepts and then write your implementation here.\n","\n","    At each step,\n","    1. Pad your input if it is packed (Unpack it)\n","    2. Reduce the input length dimension by concatenating feature dimension\n","        (Tip: Write down the shapes and understand)\n","        (i) How should  you deal with odd/even length input? \n","        (ii) How should you deal with input length array (x_lens) after truncating the input?\n","    3. Pack your input\n","    4. Pass it into LSTM layer\n","\n","    To make our implementation modular, we pass 1 layer at a time.\n","    '''\n","    \n","    def __init__(self, input_size, hidden_size):\n","        super(pBLSTM, self).__init__()\n","        # Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n","        self.blstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=config['num_layers'], batch_first=True, bidirectional=True)\n","\n","    def forward(self, x_packed): # x_packed is a PackedSequence\n","\n","        # Pad Packed Sequence \n","        if isinstance(x_packed, torch.nn.utils.rnn.PackedSequence):\n","          # pad a packed sequence after computation from the RNN\n","          # seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch first=True)\n","          x, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n","        else:\n","          x = x_packed\n","          x_lens = None\n","\n","        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n","        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n","        x, x_lens = self.trunc_reshape(x, x_lens)\n","        \n","        # Pack Padded Sequence. What output(s) would you get?\n","        if x_lens is not None:\n","          # pack a padded sequence before passing it through an RNN\n","          # packed = pack_padded_sequence(recordings, length_of_each_recording, batch_first=True, enforce_sorted=False)\n","          x_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n","        else:\n","          x_packed = x\n","\n","        # Pass the sequence through bLSTM\n","        output, (h_n, c_n) = self.blstm(x_packed)\n","\n","        # What do you return?\n","        return output\n","\n","    def trunc_reshape(self, x, x_lens): \n","        batch_size, length, features = x.shape\n","        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n","        '''\n","        If the length of the original input sequence is odd, then it is either padded out by appending\n","        an extra vector of zeros at the end, before reshaping, or trimmed to an even length by deleting\n","        the final vector. \n","        '''\n","        if (length % 2) != 0:\n","          # trim to an even length by deleting the final vector and decrementing length\n","          x = x[:, :-1, :]\n","          x_lens[-1] -= 1\n","        \n","        downsampled_length = length // 2\n","        # Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n","        x = torch.reshape(x, shape=(batch_size, downsampled_length, 2 * features))\n","        # Reduce lengths by the same downsampling factor = 2\n","        x_lens = torch.clamp(x_lens, max=downsampled_length, out=None)\n","        return x, x_lens"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:23:46.298787Z","iopub.status.busy":"2024-03-23T05:23:46.297880Z","iopub.status.idle":"2024-03-23T05:23:46.307028Z","shell.execute_reply":"2024-03-23T05:23:46.305776Z","shell.execute_reply.started":"2024-03-23T05:23:46.298751Z"},"trusted":true},"outputs":[],"source":["from torch.autograd import Variable\n","class LockedDropout(nn.Module):\n","  '''\n","  Locked dropout is a variation of dropout that is specifically designed for use in \n","  the parallel bidirectional LSTM (pBLSTM) architecture. It uses the same dropout mask \n","  for both the forward and backward layers.  This ensures that the same set of neurons \n","  are dropped out at each time step for both layers, maintaining consistency across time.\n","\n","  From https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py\n","  '''\n","  def __init__(self):\n","    super().__init__()\n","\n","  def forward(self, x, dropout=config['dropout']):\n","    if not self.training or not dropout:\n","      return x\n","    x, x_lens = pad_packed_sequence(x, batch_first=True)\n","    m = x.new(x.shape[0],1,x.shape[2]).bernoulli_(1-dropout)\n","    mask = Variable(m, requires_grad=False) / (1 - dropout)\n","    mask = mask.expand_as(x)\n","    x = mask * x\n","    # pack a padded sequence using before passing it through an RNN\n","    x_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n","    return x_packed"]},{"cell_type":"markdown","metadata":{},"source":["### Encoder"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:42:28.887471Z","iopub.status.busy":"2024-03-23T03:42:28.887088Z","iopub.status.idle":"2024-03-23T03:42:28.896863Z","shell.execute_reply":"2024-03-23T03:42:28.895815Z","shell.execute_reply.started":"2024-03-23T03:42:28.887439Z"},"trusted":true},"outputs":[],"source":["class Encoder(torch.nn.Module):\n","    '''\n","    The Encoder takes utterances as inputs and returns latent feature representations\n","    '''\n","    def __init__(self, input_size, embed_size, hidden_size):\n","        super(Encoder, self).__init__()\n","\n","        # You can use CNNs as Embedding layer to extract features. \n","        # Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n","        self.embedding = torch.nn.Sequential(\n","            PermuteBlock(), # transpose to (N, C, L) for Conv1d\n","            nn.Conv1d(input_size, embed_size, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm1d(embed_size),\n","            PermuteBlock(), # transpose back for RNN\n","        )\n","        # How many pBLSTMs are required?\n","        # Fill this up with pBLSTMs - What should the input_size be? \n","        # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n","        self.pBLSTMs = torch.nn.Sequential(\n","            pBLSTM(input_size=2*embed_size, hidden_size=hidden_size),\n","            LockedDropout(),\n","            pBLSTM(input_size=4*hidden_size, hidden_size=hidden_size),\n","            LockedDropout(),\n","        )          \n","    def forward(self, x, x_lens):\n","        # Where are x and x_lens coming from? The dataloader\n","        \n","        # Call the embedding layer\n","        x = self.embedding(x)\n","        \n","        # Pack padded sequence\n","        x = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n","\n","        # Pass Sequence through the pyramidal Bi-LSTM layer\n","        x = self.pBLSTMs(x)\n","\n","         # Pad packed sequence\n","        encoder_outputs, encoder_lens = pad_packed_sequence(x, batch_first=True)\n","\n","        return encoder_outputs, encoder_lens"]},{"cell_type":"markdown","metadata":{},"source":["### Decoder"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:23:51.162234Z","iopub.status.busy":"2024-03-23T05:23:51.161862Z","iopub.status.idle":"2024-03-23T05:23:51.170930Z","shell.execute_reply":"2024-03-23T05:23:51.169900Z","shell.execute_reply.started":"2024-03-23T05:23:51.162205Z"},"trusted":true},"outputs":[],"source":["class Decoder(torch.nn.Module):\n","\n","    def __init__(self, embed_size, output_size= 41):\n","        super().__init__()\n","\n","        self.mlp = torch.nn.Sequential(\n","            PermuteBlock(), torch.nn.BatchNorm1d(2 * embed_size), PermuteBlock(),\n","            #Use Permute Block before and after BatchNorm1d() to match the size\n","            torch.nn.Linear(2*embed_size, 2048),\n","            PermuteBlock(), torch.nn.BatchNorm1d(2048), PermuteBlock(),\n","            torch.nn.GELU(),\n","            torch.nn.Linear(2048, 2048),\n","            PermuteBlock(), torch.nn.BatchNorm1d(2048), PermuteBlock(),\n","            torch.nn.GELU(),\n","            torch.nn.Dropout(config['dropout']),\n","            torch.nn.Linear(2048, output_size)\n","        )\n","        \n","        self.softmax = torch.nn.LogSoftmax(dim=2)\n","\n","    def forward(self, encoder_out):\n","        # call your MLP\n","        out = self.mlp(encoder_out)\n","        # Think what should be the final output of the decoder for the classification \n","        out = self.softmax(out)\n","\n","        return out "]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:23:54.736844Z","iopub.status.busy":"2024-03-23T05:23:54.735945Z","iopub.status.idle":"2024-03-23T05:23:54.744379Z","shell.execute_reply":"2024-03-23T05:23:54.743497Z","shell.execute_reply.started":"2024-03-23T05:23:54.736788Z"},"trusted":true},"outputs":[],"source":["class ASRModel(torch.nn.Module):\n","\n","    def __init__(self, input_size, embed_size, hidden_size, output_size= len(PHONEMES)):\n","        super().__init__()\n","\n","        self.augmentations = torch.nn.Sequential(\n","            # Add Time Masking/ Frequency Masking\n","            # Hint: See how to use PermuteBlock() function defined above\n","            PermuteBlock(),\n","            tat.FrequencyMasking(freq_mask_param=10),\n","            tat.TimeMasking(time_mask_param=40),\n","            PermuteBlock(),\n","        )\n","        # Initialize Encoder\n","        self.encoder = Encoder(input_size=input_size, embed_size = embed_size, hidden_size=hidden_size)\n","        # Initialize Decoder\n","        self.decoder = Decoder(embed_size=hidden_size, output_size=output_size) \n","        \n","    \n","    def forward(self, x, lengths_x):\n","        if self.training:\n","            x = self.augmentations(x)\n","        encoder_out, encoder_lens   = self.encoder(x, lengths_x)\n","        decoder_out                 = self.decoder(encoder_out)\n","\n","        return decoder_out, encoder_lens"]},{"cell_type":"markdown","metadata":{},"source":["## Initialize ASR Network"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:23:58.517183Z","iopub.status.busy":"2024-03-23T05:23:58.516783Z","iopub.status.idle":"2024-03-23T05:23:58.675677Z","shell.execute_reply":"2024-03-23T05:23:58.674743Z","shell.execute_reply.started":"2024-03-23T05:23:58.517153Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ASRModel(\n","  (augmentations): Sequential(\n","    (0): PermuteBlock()\n","    (1): FrequencyMasking()\n","    (2): TimeMasking()\n","    (3): PermuteBlock()\n","  )\n","  (encoder): Encoder(\n","    (embedding): Sequential(\n","      (0): PermuteBlock()\n","      (1): Conv1d(27, 27, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n","      (2): BatchNorm1d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (3): PermuteBlock()\n","    )\n","    (pBLSTMs): Sequential(\n","      (0): pBLSTM(\n","        (blstm): LSTM(54, 256, num_layers=3, batch_first=True, bidirectional=True)\n","      )\n","      (1): LockedDropout()\n","      (2): pBLSTM(\n","        (blstm): LSTM(1024, 256, num_layers=3, batch_first=True, bidirectional=True)\n","      )\n","      (3): LockedDropout()\n","    )\n","  )\n","  (decoder): Decoder(\n","    (mlp): Sequential(\n","      (0): PermuteBlock()\n","      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): PermuteBlock()\n","      (3): Linear(in_features=512, out_features=2048, bias=True)\n","      (4): PermuteBlock()\n","      (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (6): PermuteBlock()\n","      (7): GELU(approximate='none')\n","      (8): Linear(in_features=2048, out_features=2048, bias=True)\n","      (9): PermuteBlock()\n","      (10): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (11): PermuteBlock()\n","      (12): GELU(approximate='none')\n","      (13): Dropout(p=0.2, inplace=False)\n","      (14): Linear(in_features=2048, out_features=41, bias=True)\n","    )\n","    (softmax): LogSoftmax(dim=2)\n","  )\n",")\n"]}],"source":["model = ASRModel(\n","    input_size  = 27, #TODO,\n","    embed_size  =  27, #TODO\n","    hidden_size= 256,\n","    output_size = len(PHONEMES)\n",").to(device)\n","print(model)\n","#summary(model, x.to(device), lx)\n","#summary(model, input_size=(27, None)) "]},{"cell_type":"markdown","metadata":{},"source":["# Training Config\n","Initialize Loss Criterion, Optimizer, CTC Beam Decoder, Scheduler, Scaler (Mixed-Precision), etc."]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T06:12:56.195387Z","iopub.status.busy":"2024-03-23T06:12:56.194596Z","iopub.status.idle":"2024-03-23T06:12:56.204160Z","shell.execute_reply":"2024-03-23T06:12:56.203097Z","shell.execute_reply.started":"2024-03-23T06:12:56.195353Z"},"trusted":true},"outputs":[],"source":["#TODO\n","\n","\n","criterion = nn.CTCLoss(blank= CMUdict.index(''))# Define CTC loss as the criterion. How would the losses be reduced?\n","# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n","# Refer to the handout for hints\n","\n","# optimizer =  torch.optim.AdamW(...) # What goes in here?\n","optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n","\n","# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n","# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n","decoder = CTCBeamDecoder(labels=LABELS, beam_width=config['beam_width'], num_processes= 4, log_probs_input=True)#TODO \n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode= config['sc_mode'], factor= config['factor'], patience= config['patience'], threshold= config['sc_threshold'], threshold_mode= config['threshold mode'],  verbose=True)#TODO\n","\n","# Mixed Precision, if you need it\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"markdown","metadata":{},"source":["# Decode Prediction"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T06:12:59.493539Z","iopub.status.busy":"2024-03-23T06:12:59.493164Z","iopub.status.idle":"2024-03-23T06:12:59.505612Z","shell.execute_reply":"2024-03-23T06:12:59.504462Z","shell.execute_reply.started":"2024-03-23T06:12:59.493508Z"},"trusted":true},"outputs":[],"source":["def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n","    \n","    # TODO: look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.\n","    output = torch.transpose(output, 0, 1) \n","    output, _, _, out_seq_len = decoder.decode(output, seq_lens= output_lens) #lengths - list of lengths\n","\n","    pred_strings = []\n","    \n","    for i in range(output_lens.shape[0]):\n","        #TODO: Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.\n","        logits=output[i][0][:out_seq_len[i][0]]\n","        # print('index',i)\n","        # print('logits',max(logits))\n","        try: pred_strings.append(''.join([PHONEME_MAP[i] for i in logits]))\n","        except: \n","          print(logits)\n","          return\n","    return pred_strings\n","\n","def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers\n","    \n","    dist            = 0\n","    batch_size      = label.shape[0]\n","\n","    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n","    # print(pred_strings)\n","    for i in range(batch_size):\n","        # TODO: Get predicted string and label string for each element in the batch\n","        pred_string = pred_strings[i]#TODO\n","        label_string = ''.join(PHONEME_MAP[l] for l in label[i][:label_lens[i]])#TODO\n","        dist += Levenshtein.distance(pred_string, label_string)\n","\n","    dist /= batch_size # TODO: Uncomment this, but think about why we are doing this\n","    return dist"]},{"cell_type":"markdown","metadata":{},"source":["# Test Implementation"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T06:13:02.575760Z","iopub.status.busy":"2024-03-23T06:13:02.575065Z","iopub.status.idle":"2024-03-23T06:13:04.239859Z","shell.execute_reply":"2024-03-23T06:13:04.238760Z","shell.execute_reply.started":"2024-03-23T06:13:02.575725Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 573, 41])\n","torch.Size([573, 64, 41]) torch.Size([64, 241])\n","tensor(0.3120, device='cuda:0', grad_fn=<MeanBackward0>)\n","3.140625\n"]}],"source":["model.eval()\n","for i, data in enumerate(val_loader, 0):\n","    x, y, lx, ly = data\n","    x, y = x.to(device), y.to(device)\n","    h, lh = model(x, lx)\n","    print(h.shape)\n","    h = torch.permute(h, (1, 0, 2))\n","    print(h.shape, y.shape)\n","    loss = criterion(h, y, lh, ly)\n","    print(loss)\n","\n","    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n","\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["# WandB\n","\n","You will need to fetch your api key from wandb.ai"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:24:21.577577Z","iopub.status.busy":"2024-03-23T05:24:21.576890Z","iopub.status.idle":"2024-03-23T05:24:21.665785Z","shell.execute_reply":"2024-03-23T05:24:21.664763Z","shell.execute_reply.started":"2024-03-23T05:24:21.577536Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login(key=\"af28e15472d2b0ef281fdacbb101cc157ad1b80d\") #API Key is in your wandb account, under settings (wandb.ai/settings)"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:24:25.239850Z","iopub.status.busy":"2024-03-23T05:24:25.239462Z","iopub.status.idle":"2024-03-23T05:24:27.317192Z","shell.execute_reply":"2024-03-23T05:24:27.315710Z","shell.execute_reply.started":"2024-03-23T05:24:25.239802Z"},"trusted":true},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.16.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240323_052425-av362bvm</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Resuming run <strong><a href='https://wandb.ai/lniyiteg/hw3p2-ablations/runs/av362bvm' target=\"_blank\">early-submission</a></strong> to <a href='https://wandb.ai/lniyiteg/hw3p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/lniyiteg/hw3p2-ablations' target=\"_blank\">https://wandb.ai/lniyiteg/hw3p2-ablations</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/lniyiteg/hw3p2-ablations/runs/av362bvm' target=\"_blank\">https://wandb.ai/lniyiteg/hw3p2-ablations/runs/av362bvm</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["run = wandb.init(\n","    name = \"early-submission\", ## Wandb creates random run names if you skip this field\n","    #reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    id = \"av362bvm\", #Insert specific run id here if you want to resume a previous run\n","    resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"HW3P2-ablations\", ### Project should be created in your wandb account\n","    config = config ### Wandb Config for your run\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Train Functions"]},{"cell_type":"markdown","metadata":{},"source":["## Training Setup"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:24:29.455280Z","iopub.status.busy":"2024-03-23T05:24:29.454620Z","iopub.status.idle":"2024-03-23T05:24:29.474470Z","shell.execute_reply":"2024-03-23T05:24:29.473443Z","shell.execute_reply.started":"2024-03-23T05:24:29.455241Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","def train_model(model, train_loader, criterion, optimizer):\n","    \n","    model.train()\n","    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n","\n","    total_loss = 0\n","\n","    for i, data in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        x, y, lx, ly = data\n","        x, y = x.to(device), y.to(device)\n","\n","        with torch.cuda.amp.autocast():     \n","            h, lh = model(x, lx)\n","            h = torch.permute(h, (1, 0, 2))\n","            loss = criterion(h, y, lh, ly)\n","\n","        total_loss += loss.item()\n","\n","        batch_bar.set_postfix(\n","            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n","            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n","\n","        batch_bar.update() # Update tqdm bar\n","\n","        # Another couple things you need for FP16. \n","        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n","        scaler.step(optimizer) # This is a replacement for optimizer.step()\n","        scaler.update() # This is something added just for FP16\n","\n","        del x, y, lx, ly, h, lh, loss \n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close() # You need this to close the tqdm bar\n","    \n","    return total_loss / len(train_loader)\n","\n","\n","def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n","\n","    model.eval()\n","    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n","\n","    total_loss = 0\n","    vdist = 0\n","\n","    for i, data in enumerate(val_loader):\n","\n","        x, y, lx, ly = data\n","        x, y = x.to(device), y.to(device)\n","\n","        with torch.inference_mode():\n","            h, lh = model(x, lx)\n","            h = torch.permute(h, (1, 0, 2))\n","            loss = criterion(h, y, lh, ly)\n","\n","        total_loss += float(loss)\n","        # vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n","        vdist += calculate_levenshtein(h, y, lh, ly, decoder, phoneme_map)\n","\n","        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n","\n","        batch_bar.update()\n","    \n","        del x, y, lx, ly, h, lh, loss\n","        torch.cuda.empty_cache()\n","        \n","    batch_bar.close()\n","    total_loss = total_loss/len(val_loader)\n","    val_dist = vdist/len(val_loader)\n","    return total_loss, val_dist"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T06:13:19.855670Z","iopub.status.busy":"2024-03-23T06:13:19.855253Z","iopub.status.idle":"2024-03-23T06:13:21.606021Z","shell.execute_reply":"2024-03-23T06:13:21.604938Z","shell.execute_reply.started":"2024-03-23T06:13:19.855636Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 664, 41])\n","torch.Size([664, 64, 41]) torch.Size([64, 260])\n","tensor(0.3418, device='cuda:0', grad_fn=<MeanBackward0>)\n","4.25\n"]}],"source":["# test code to check shapes\n","\n","model.eval()\n","for i, data in enumerate(val_loader, 0):\n","    x, y, lx, ly = data\n","    x, y = x.to(device), y.to(device)\n","    h, lh = model(x, lx)\n","    print(h.shape)\n","    h = torch.permute(h, (1, 0, 2))\n","    print(h.shape, y.shape)\n","    loss = criterion(h, y, lh, ly)\n","    print(loss)\n","\n","    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n","\n","    break"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:24:45.781175Z","iopub.status.busy":"2024-03-23T05:24:45.780226Z","iopub.status.idle":"2024-03-23T05:24:45.791572Z","shell.execute_reply":"2024-03-23T05:24:45.790182Z","shell.execute_reply.started":"2024-03-23T05:24:45.781136Z"},"trusted":true},"outputs":[],"source":["def save_model(model, optimizer, scheduler, metric, epoch, path):\n","    torch.save(\n","        {'model_state_dict'         : model.state_dict(),\n","         'optimizer_state_dict'     : optimizer.state_dict(),\n","         'scheduler_state_dict'     : scheduler.state_dict(),\n","         metric[0]                  : metric[1], \n","         'epoch'                    : epoch}, \n","         path\n","    )\n","\n","def load_model(path, model, metric= 'valid_dist', optimizer= None, scheduler= None):\n","\n","    checkpoint = torch.load(path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    if optimizer != None:\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    if scheduler != None:\n","        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","        \n","    epoch   = checkpoint['epoch']\n","    metric  = checkpoint[metric]\n","\n","    return [model, optimizer, scheduler, epoch, metric]"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:24:48.899976Z","iopub.status.busy":"2024-03-23T05:24:48.899589Z","iopub.status.idle":"2024-03-23T05:24:48.906760Z","shell.execute_reply":"2024-03-23T05:24:48.905666Z","shell.execute_reply.started":"2024-03-23T05:24:48.899945Z"},"trusted":true},"outputs":[],"source":["# This is for checkpointing, if you're doing it over multiple sessions\n","\n","last_epoch_completed = 0\n","start = last_epoch_completed\n","end = config[\"epochs\"]\n","best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n","epoch_model_path = \"/kaggle/working/checkpoint.pth\"#TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n","best_model_path = \"/kaggle/working/best_checkpoint.pth\"#TODO set best model path "]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T06:13:34.427420Z","iopub.status.busy":"2024-03-23T06:13:34.427051Z","iopub.status.idle":"2024-03-23T06:13:34.439232Z","shell.execute_reply":"2024-03-23T06:13:34.438054Z","shell.execute_reply.started":"2024-03-23T06:13:34.427389Z"},"trusted":true},"outputs":[],"source":["\n","#TODO: Please complete the training loop\n","def experiment(start_epoch=0):\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    best_lev_dist = float('inf')\n","    \n","    for epoch in range(start_epoch, config['epochs']):\n","\n","        print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n","    \n","        curr_lr = float(optimizer.param_groups[0]['lr']) #TODO\n","\n","        train_loss              = train_model(model, train_loader, criterion, optimizer)#TODO\n","        valid_loss, valid_dist  = validate_model(model, val_loader, decoder, phoneme_map= LABELS)#TODO\n","        scheduler.step(valid_dist)\n","\n","        print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n","        print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n","\n","\n","        wandb.log({\n","            'train_loss': train_loss,  \n","            'valid_dist': valid_dist, \n","           'valid_loss': valid_loss, \n","           'lr'        : curr_lr\n","            })\n","    \n","        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n","        wandb.save(epoch_model_path)\n","        print(\"Saved epoch model\")\n","\n","        if valid_dist <= best_lev_dist:\n","            best_lev_dist = valid_dist\n","            save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n","            wandb.save(best_model_path)\n","            print(\"Saved best model\")\n","      # You may find it interesting to exlplore Wandb Artifcats to version your models\n","    run.finish()"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T06:13:42.344239Z","iopub.status.busy":"2024-03-23T06:13:42.343867Z","iopub.status.idle":"2024-03-23T06:19:49.837794Z","shell.execute_reply":"2024-03-23T06:19:49.836720Z","shell.execute_reply.started":"2024-03-23T06:13:42.344208Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["loading checkpoint...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 73/73\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 00014: reducing learning rate of group 0 to 6.1035e-08.\n","\tTrain Loss 0.0539\t Learning Rate 0.0000001\n","\tVal Dist 3.9983%\t Val Loss 0.2988\n","Saved epoch model\n","Saved best model\n"]},{"name":"stderr","output_type":"stream","text":["wandb: WARNING Ensure read and write access to run files dir: /kaggle/working/wandb/run-20240323_052425-av362bvm/files, control this via the WANDB_DIR env var. See https://docs.wandb.ai/guides/track/environment-variables\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='341.578 MB of 341.578 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>██▄▄▂▂▁▄</td></tr><tr><td>train_loss</td><td>█▅▅▁▃▆█▇</td></tr><tr><td>valid_dist</td><td>▃▂▁█▄▆▇▅</td></tr><tr><td>valid_loss</td><td>▆█▅▁▇▆█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.05388</td></tr><tr><td>valid_dist</td><td>3.99826</td></tr><tr><td>valid_loss</td><td>0.29879</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">early-submission</strong> at: <a href='https://wandb.ai/lniyiteg/hw3p2-ablations/runs/av362bvm' target=\"_blank\">https://wandb.ai/lniyiteg/hw3p2-ablations/runs/av362bvm</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240323_052425-av362bvm/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["#wandb.init(project=\"hw2p2 ablations\", entity=\"01 face verification & face classification\")\n","\n","def load_checkpoint(filepath):\n","    checkpoint = torch.load(filepath)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","    epoch = checkpoint['epoch']\n","    val_dist = checkpoint['valid_dist']\n","    return model, optimizer, scheduler, epoch, val_dist\n","\n","\n","# Load the checkpoint if available\n","start_epoch = 0\n","checkpoint_val_dist = './best_checkpoint.pth'\n","\n","\n","if os.path.exists(checkpoint_val_dist):\n","    print(\"loading checkpoint...\")\n","    model, optimizer, scheduler, starting_epoch, best_class_acc = load_checkpoint(checkpoint_val_dist )\n","    experiment(start_epoch= starting_epoch)\n","else:\n","    print(\"the checkpoint paths does not exist\")\n","    print(\"starting from scratch\")\n","    experiment()\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T20:45:55.868737Z","iopub.status.busy":"2024-03-22T20:45:55.868101Z","iopub.status.idle":"2024-03-22T21:39:27.455719Z","shell.execute_reply":"2024-03-22T21:39:27.454923Z","shell.execute_reply.started":"2024-03-22T20:45:55.868701Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Epoch: 62/70\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0530\t Learning Rate 0.0000039\n","\tVal Dist 3.9909%\t Val Loss 0.3047\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"]},{"name":"stdout","output_type":"stream","text":["Saved epoch model\n","Saved best model\n","\n","Epoch: 63/70\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0512\t Learning Rate 0.0000039\n","\tVal Dist 4.0232%\t Val Loss 0.3081\n","Saved epoch model\n","\n","Epoch: 64/70\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 00003: reducing learning rate of group 0 to 1.9531e-06.\n","\tTrain Loss 0.0529\t Learning Rate 0.0000039\n","\tVal Dist 4.0037%\t Val Loss 0.3193\n","Saved epoch model\n","\n","Epoch: 65/70\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0537\t Learning Rate 0.0000020\n","\tVal Dist 3.9685%\t Val Loss 0.3021\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 66/70\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0480\t Learning Rate 0.0000020\n","\tVal Dist 4.0098%\t Val Loss 0.3084\n","Saved epoch model\n","\n","Epoch: 67/70\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0521\t Learning Rate 0.0000020\n","\tVal Dist 3.9605%\t Val Loss 0.3212\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 68/70\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0570\t Learning Rate 0.0000020\n","\tVal Dist 3.9663%\t Val Loss 0.3094\n","Saved epoch model\n","\n","Epoch: 69/70\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0510\t Learning Rate 0.0000020\n","\tVal Dist 3.9583%\t Val Loss 0.3051\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 70/70\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0570\t Learning Rate 0.0000020\n","\tVal Dist 4.0104%\t Val Loss 0.3137\n","Saved epoch model\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='295.243 MB of 341.578 MB uploaded\\r'), FloatProgress(value=0.8643489894333549, max…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: WARNING Ensure read and write access to run files dir: /kaggle/working/wandb/run-20240322_204448-av362bvm/files, control this via the WANDB_DIR env var. See https://docs.wandb.ai/guides/track/environment-variables\n"]},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>███▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▅▄▅▅▁▄█▃█</td></tr><tr><td>valid_dist</td><td>▅█▆▂▇▁▂▁▇</td></tr><tr><td>valid_loss</td><td>▂▃▇▁▃█▄▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.05698</td></tr><tr><td>valid_dist</td><td>4.01044</td></tr><tr><td>valid_loss</td><td>0.31369</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">early-submission</strong> at: <a href='https://wandb.ai/lniyiteg/hw3p2-ablations/runs/av362bvm' target=\"_blank\">https://wandb.ai/lniyiteg/hw3p2-ablations/runs/av362bvm</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240322_204448-av362bvm/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["try:\n","    artifact1 = wandb.restore(\"best_checkpoint.pth\")\n","    gc.collect()\n","    wandb.watch(model, log=\"all\")\n","    # Check if the artifact is valid\n","    if artifact1 is not None:\n","        best_val_acc = 0.0\n","        checkpoint_path = artifact1.name\n","\n","            # Load checkpoint if exists\n","        if os.path.exists(checkpoint_path):\n","            checkpoint = torch.load(checkpoint_path)\n","            model.load_state_dict(checkpoint['model_state_dict'])\n","            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","            start_epoch = checkpoint.get('epoch', 1)\n","            best_val_acc = checkpoint.get('val_acc', 0)\n","            experiment(start_epoch= start_epoch)\n","            \n","\n","\n","        else:\n","            start_epoch = 1\n","            best_val_acc = 0.0\n","except Exception as ex:\n","    print(f\"Exception is thrown because {ex}\")\n","    pass"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T03:43:57.414951Z","iopub.status.busy":"2024-03-23T03:43:57.414215Z","iopub.status.idle":"2024-03-23T05:09:00.020122Z","shell.execute_reply":"2024-03-23T05:09:00.019363Z","shell.execute_reply.started":"2024-03-23T03:43:57.414920Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Epoch: 62/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0529\t Learning Rate 0.0000039\n","\tVal Dist 3.9690%\t Val Loss 0.3101\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"]},{"name":"stdout","output_type":"stream","text":["Saved epoch model\n","Saved best model\n","\n","Epoch: 63/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0515\t Learning Rate 0.0000039\n","\tVal Dist 3.9879%\t Val Loss 0.3031\n","Saved epoch model\n","\n","Epoch: 64/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 00003: reducing learning rate of group 0 to 1.9531e-06.\n","\tTrain Loss 0.0481\t Learning Rate 0.0000039\n","\tVal Dist 3.9827%\t Val Loss 0.3115\n","Saved epoch model\n","\n","Epoch: 65/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0510\t Learning Rate 0.0000020\n","\tVal Dist 3.9892%\t Val Loss 0.3175\n","Saved epoch model\n","\n","Epoch: 66/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 00005: reducing learning rate of group 0 to 9.7656e-07.\n","\tTrain Loss 0.0511\t Learning Rate 0.0000020\n","\tVal Dist 3.9891%\t Val Loss 0.3064\n","Saved epoch model\n","\n","Epoch: 67/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0518\t Learning Rate 0.0000010\n","\tVal Dist 4.0264%\t Val Loss 0.3183\n","Saved epoch model\n","\n","Epoch: 68/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 00007: reducing learning rate of group 0 to 4.8828e-07.\n","\tTrain Loss 0.0515\t Learning Rate 0.0000010\n","\tVal Dist 4.0018%\t Val Loss 0.2919\n","Saved epoch model\n","\n","Epoch: 69/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0545\t Learning Rate 0.0000005\n","\tVal Dist 4.0162%\t Val Loss 0.3036\n","Saved epoch model\n","\n","Epoch: 70/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 00009: reducing learning rate of group 0 to 2.4414e-07.\n","\tTrain Loss 0.0479\t Learning Rate 0.0000005\n","\tVal Dist 3.9805%\t Val Loss 0.3114\n","Saved epoch model\n","\n","Epoch: 71/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0533\t Learning Rate 0.0000002\n","\tVal Dist 3.9638%\t Val Loss 0.3078\n","Saved epoch model\n","Saved best model\n","\n","Epoch: 72/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0535\t Learning Rate 0.0000002\n","\tVal Dist 3.9891%\t Val Loss 0.2987\n","Saved epoch model\n","\n","Epoch: 73/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 00012: reducing learning rate of group 0 to 1.2207e-07.\n","\tTrain Loss 0.0480\t Learning Rate 0.0000002\n","\tVal Dist 3.9872%\t Val Loss 0.3051\n","Saved epoch model\n","\n","Epoch: 74/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["\tTrain Loss 0.0494\t Learning Rate 0.0000001\n","\tVal Dist 3.9751%\t Val Loss 0.3134\n","Saved epoch model\n","\n","Epoch: 75/75\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                  \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 00014: reducing learning rate of group 0 to 6.1035e-08.\n","\tTrain Loss 0.0539\t Learning Rate 0.0000001\n","\tVal Dist 4.0059%\t Val Loss 0.2949\n","Saved epoch model\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='341.578 MB of 341.578 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: WARNING Ensure read and write access to run files dir: /kaggle/working/wandb/run-20240323_034315-av362bvm/files, control this via the WANDB_DIR env var. See https://docs.wandb.ai/guides/track/environment-variables\n"]},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>███▄▄▃▃▂▂▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▆▅▁▄▄▅▅█▁▇▇▁▃▇</td></tr><tr><td>valid_dist</td><td>▂▄▃▄▄█▅▇▃▁▄▄▂▆</td></tr><tr><td>valid_loss</td><td>▆▄▆█▅█▁▄▆▅▃▅▇▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.05388</td></tr><tr><td>valid_dist</td><td>4.00589</td></tr><tr><td>valid_loss</td><td>0.29493</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">early-submission</strong> at: <a href='https://wandb.ai/lniyiteg/hw3p2-ablations/runs/av362bvm' target=\"_blank\">https://wandb.ai/lniyiteg/hw3p2-ablations/runs/av362bvm</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240323_034315-av362bvm/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["try:\n","    artifact1 = wandb.restore(\"best_checkpoint.pth\")\n","    gc.collect()\n","    wandb.watch(model, log=\"all\")\n","    # Check if the artifact is valid\n","    if artifact1 is not None:\n","        best_val_acc = 0.0\n","        checkpoint_path = artifact1.name\n","\n","            # Load checkpoint if exists\n","        if os.path.exists(checkpoint_path):\n","            checkpoint = torch.load(checkpoint_path)\n","            model.load_state_dict(checkpoint['model_state_dict'])\n","            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","            start_epoch = checkpoint.get('epoch', 1)\n","            best_val_acc = checkpoint.get('val_acc', 0)\n","            experiment(start_epoch= start_epoch)\n","            \n","\n","\n","        else:\n","            start_epoch = 1\n","            best_val_acc = 0.0\n","except Exception as ex:\n","    print(f\"Exception is thrown because {ex}\")\n","    pass"]},{"cell_type":"markdown","metadata":{},"source":["# Generate Predictions and Submit to Kaggle"]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T06:20:09.642967Z","iopub.status.busy":"2024-03-23T06:20:09.642554Z","iopub.status.idle":"2024-03-23T06:20:36.153407Z","shell.execute_reply":"2024-03-23T06:20:36.152305Z","shell.execute_reply.started":"2024-03-23T06:20:09.642932Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing\n"]},{"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n","  2%|▏         | 1/41 [00:00<00:34,  1.18it/s]\u001b[A\n","  5%|▍         | 2/41 [00:01<00:29,  1.32it/s]\u001b[A\n","  7%|▋         | 3/41 [00:02<00:29,  1.27it/s]\u001b[A\n"," 10%|▉         | 4/41 [00:02<00:26,  1.39it/s]\u001b[A\n"," 12%|█▏        | 5/41 [00:03<00:25,  1.41it/s]\u001b[A\n"," 15%|█▍        | 6/41 [00:04<00:22,  1.56it/s]\u001b[A\n"," 17%|█▋        | 7/41 [00:04<00:18,  1.88it/s]\u001b[A\n"," 20%|█▉        | 8/41 [00:05<00:20,  1.63it/s]\u001b[A\n"," 22%|██▏       | 9/41 [00:06<00:21,  1.51it/s]\u001b[A\n"," 24%|██▍       | 10/41 [00:06<00:21,  1.44it/s]\u001b[A\n"," 27%|██▋       | 11/41 [00:07<00:17,  1.68it/s]\u001b[A\n"," 29%|██▉       | 12/41 [00:07<00:16,  1.71it/s]\u001b[A\n"," 32%|███▏      | 13/41 [00:08<00:15,  1.83it/s]\u001b[A\n"," 34%|███▍      | 14/41 [00:08<00:16,  1.67it/s]\u001b[A\n"," 37%|███▋      | 15/41 [00:09<00:16,  1.54it/s]\u001b[A\n"," 39%|███▉      | 16/41 [00:10<00:17,  1.45it/s]\u001b[A\n"," 41%|████▏     | 17/41 [00:11<00:17,  1.37it/s]\u001b[A\n"," 44%|████▍     | 18/41 [00:11<00:16,  1.41it/s]\u001b[A\n"," 46%|████▋     | 19/41 [00:12<00:12,  1.69it/s]\u001b[A\n"," 49%|████▉     | 20/41 [00:13<00:13,  1.53it/s]\u001b[A\n"," 51%|█████     | 21/41 [00:13<00:13,  1.47it/s]\u001b[A\n"," 54%|█████▎    | 22/41 [00:14<00:13,  1.43it/s]\u001b[A\n"," 56%|█████▌    | 23/41 [00:15<00:12,  1.41it/s]\u001b[A\n"," 59%|█████▊    | 24/41 [00:15<00:09,  1.71it/s]\u001b[A\n"," 61%|██████    | 25/41 [00:16<00:10,  1.52it/s]\u001b[A\n"," 63%|██████▎   | 26/41 [00:16<00:09,  1.57it/s]\u001b[A\n"," 66%|██████▌   | 27/41 [00:17<00:08,  1.74it/s]\u001b[A\n"," 68%|██████▊   | 28/41 [00:17<00:06,  1.88it/s]\u001b[A\n"," 71%|███████   | 29/41 [00:18<00:06,  1.73it/s]\u001b[A\n"," 73%|███████▎  | 30/41 [00:18<00:05,  1.89it/s]\u001b[A\n"," 76%|███████▌  | 31/41 [00:19<00:05,  1.78it/s]\u001b[A\n"," 78%|███████▊  | 32/41 [00:20<00:05,  1.61it/s]\u001b[A\n"," 80%|████████  | 33/41 [00:21<00:05,  1.59it/s]\u001b[A\n"," 83%|████████▎ | 34/41 [00:21<00:04,  1.61it/s]\u001b[A\n"," 85%|████████▌ | 35/41 [00:22<00:03,  1.65it/s]\u001b[A\n"," 88%|████████▊ | 36/41 [00:23<00:03,  1.44it/s]\u001b[A\n"," 90%|█████████ | 37/41 [00:23<00:02,  1.40it/s]\u001b[A\n"," 93%|█████████▎| 38/41 [00:24<00:01,  1.54it/s]\u001b[A\n"," 95%|█████████▌| 39/41 [00:24<00:01,  1.60it/s]\u001b[A\n"," 98%|█████████▊| 40/41 [00:25<00:00,  1.59it/s]\u001b[A\n","100%|██████████| 41/41 [00:26<00:00,  1.55it/s]\u001b[A\n"]}],"source":["#TODO: Make predictions\n","\n","# Follow the steps below:\n","# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n","# 2. Get prediction string by decoding the results of the beam decoder\n","\n","TEST_BEAM_WIDTH = 2 #TODO\n","\n","test_decoder    = CTCBeamDecoder(labels=LABELS, beam_width=TEST_BEAM_WIDTH, log_probs_input=True)#TODO\n","results = []\n","\n","model.eval()\n","print(\"Testing\")\n","for data in tqdm(test_loader):\n","\n","    x, lx   = data\n","    x       = x.to(device)\n","\n","    with torch.no_grad():\n","        h, lh = model(x, lx)\n","        h = torch.permute(h, (1, 0, 2))\n","    prediction_string= decode_prediction(h, lh, test_decoder, LABELS)# TODO call decode_prediction \n","    #TODO save the output in results array.\n","    results.extend(prediction_string)\n","    \n","    del x, lx, h, lh\n","    torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:10:31.054956Z","iopub.status.busy":"2024-03-23T05:10:31.054499Z","iopub.status.idle":"2024-03-23T05:10:31.059867Z","shell.execute_reply":"2024-03-23T05:10:31.058867Z","shell.execute_reply.started":"2024-03-23T05:10:31.054923Z"},"trusted":true},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T05:10:33.820848Z","iopub.status.busy":"2024-03-23T05:10:33.820453Z","iopub.status.idle":"2024-03-23T05:10:33.879512Z","shell.execute_reply":"2024-03-23T05:10:33.878624Z","shell.execute_reply.started":"2024-03-23T05:10:33.820797Z"},"trusted":true},"outputs":[],"source":["#'/kaggle/working/11-785-s24-hw3p2/'\n","# data_dir = \"/kaggle/working/11-785-s24-hw3p2/\" + \"/test-clean/random_submission.csv\"\n","data_dir = f\"{root}/test-clean/random_submission.csv\"\n","df = pd.read_csv(data_dir)\n","df.label = results\n","df.to_csv('submission.csv', index = False)"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2024-03-23T06:20:44.242062Z","iopub.status.busy":"2024-03-23T06:20:44.241656Z","iopub.status.idle":"2024-03-23T06:20:48.009257Z","shell.execute_reply":"2024-03-23T06:20:48.008281Z","shell.execute_reply.started":"2024-03-23T06:20:44.242029Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.7 / client 1.5.8)\n","100%|█████████████████████████████████████████| 209k/209k [00:01<00:00, 206kB/s]\n","Successfully submitted to HW3P2_ASR-S24"]}],"source":["!kaggle competitions submit -c hw3p2asr-s24 -f submission.csv -m \"I made it!\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Run this cell to download the file\n","!kaggle competitions download -c hw3p2asr-s24 -f test-clean/random_submission.csv\n","\n","# The downloaded file will be saved in the current directory\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7906338,"sourceId":72227,"sourceType":"competition"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
